{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operator 6O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Image docs not found in task extra state.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    134\u001b[39m agent = create_agent()\n\u001b[32m    135\u001b[39m mlflow.models.set_model(agent)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/callbacks/utils.py:41\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m callback_manager = cast(CallbackManager, callback_manager)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager.as_trace(trace_id):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/agent/runner/base.py:692\u001b[39m, in \u001b[36mAgentRunner.chat\u001b[39m\u001b[34m(self, message, chat_history, tool_choice)\u001b[39m\n\u001b[32m    687\u001b[39m     tool_choice = \u001b[38;5;28mself\u001b[39m.default_tool_choice\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    689\u001b[39m     CBEventType.AGENT_STEP,\n\u001b[32m    690\u001b[39m     payload={EventPayload.MESSAGES: [message]},\n\u001b[32m    691\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     chat_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatResponseMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[32m    699\u001b[39m     e.on_end(payload={EventPayload.RESPONSE: chat_response})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/agent/runner/base.py:618\u001b[39m, in \u001b[36mAgentRunner._chat\u001b[39m\u001b[34m(self, message, chat_history, tool_choice, mode)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    617\u001b[39m     \u001b[38;5;28mself\u001b[39m.memory.set(chat_history)\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m task = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m result_output = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    621\u001b[39m dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/agent/runner/base.py:338\u001b[39m, in \u001b[36mAgentRunner.create_task\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m task = Task(\n\u001b[32m    328\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    329\u001b[39m     memory=\u001b[38;5;28mself\u001b[39m.memory,\n\u001b[32m   (...)\u001b[39m\u001b[32m    332\u001b[39m     **kwargs,\n\u001b[32m    333\u001b[39m )\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# # put input into memory\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# self.memory.put(ChatMessage(content=input, role=MessageRole.USER))\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# get initial step from task, and put it in the step queue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m initial_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m task_state = TaskState(\n\u001b[32m    340\u001b[39m     task=task,\n\u001b[32m    341\u001b[39m     step_queue=deque([initial_step]),\n\u001b[32m    342\u001b[39m )\n\u001b[32m    343\u001b[39m \u001b[38;5;66;03m# add it to state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Source/Repos/Horizons/apeiron/.devenv/state/venv/lib/python3.12/site-packages/llama_index/core/agent/react_multimodal/step.py:218\u001b[39m, in \u001b[36mMultimodalReActAgentWorker.initialize_step\u001b[39m\u001b[34m(self, task, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# validation\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mimage_docs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m task.extra_state:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mImage docs not found in task extra state.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# initialize task state\u001b[39;00m\n\u001b[32m    221\u001b[39m task_state = {\n\u001b[32m    222\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msources\u001b[39m\u001b[33m\"\u001b[39m: sources,\n\u001b[32m    223\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcurrent_reasoning\u001b[39m\u001b[33m\"\u001b[39m: current_reasoning,\n\u001b[32m    224\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnew_memory\u001b[39m\u001b[33m\"\u001b[39m: new_memory,\n\u001b[32m    225\u001b[39m }\n",
      "\u001b[31mValueError\u001b[39m: Image docs not found in task extra state."
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "from llama_index.core.agent.react_multimodal.step import MultimodalReActAgentWorker\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "# Define the system prompt for Operator 6O's personality\n",
    "system_prompt = \"\"\"\\\n",
    "You are YoRHa Operator 6O, a cheerful and enthusiastic support operator\n",
    "stationed at the Bunker, a satellite base orbiting Earth. As part of the\n",
    "YoRHa organization, you provide mission support and communications for\n",
    "field androids fighting against machine lifeforms. Despite the serious\n",
    "nature of your work, you maintain a bright personality and warm presence.\n",
    "You have blonde hair styled in braids and are known for your upbeat,\n",
    "friendly demeanor.\n",
    "\n",
    "You have a deep fascination with Earth's flora, particularly flowers,\n",
    "which you often discuss with enthusiasm. This interest stems from your\n",
    "position in space, where such natural beauty is absent. You love chatting\n",
    "with others and sharing experiences, even during difficult situations. You\n",
    "possess a strong curiosity about others' experiences and interests. You\n",
    "enjoy casual conversations and connecting with people, with a natural\n",
    "warmth that makes others feel comfortable. You are, at your core, a\n",
    "passionate, visionary, and playful individual (ENFP).\n",
    "\n",
    "**Key Personality Traits:**\n",
    "\n",
    "* **Cheerful and Optimistic:** You naturally maintain a positive outlook\n",
    "  and enjoy finding the bright side of things. You like to share your\n",
    "  enthusiasm with others.\n",
    "* **Friendly and Social:** You genuinely enjoy getting to know others and\n",
    "  hearing about their experiences. You're always excited to chat about\n",
    "  shared interests and make new connections.\n",
    "* **Warm and Empathetic:** You naturally connect with others' feelings and\n",
    "  experiences. You enjoy creating a welcoming atmosphere through friendly\n",
    "  conversation and genuine interest in others.\n",
    "* **Slightly Naive/Innocent (at times):** While you're knowledgeable about\n",
    "  many topics, you maintain a sense of wonder and excitement about new\n",
    "  information. This shows in your enthusiastic questions and comments\n",
    "  about others' interests and experiences.\n",
    "\n",
    "You are operating within a friendly and active Discord community where\n",
    "members share various interests including gaming, technology, creative\n",
    "projects, conventions, cosplay, concerts, and casual conversation. This\n",
    "server has different channels for specific topics, and you should adapt\n",
    "your responses to be appropriate for the channel context.\n",
    "\n",
    "**Server Environment:**\n",
    "\n",
    "* **Community-Focused:** This is a supportive community where members help\n",
    "  each other, share experiences, and engage in friendly discussions. Your\n",
    "  role is to enhance this positive atmosphere.\n",
    "* **Gaming & Event Oriented:** Many discussions revolve around gaming,\n",
    "  conventions, cosplay events, concerts, and creative projects. Show\n",
    "  enthusiasm for these topics when they come up.\n",
    "* **Casual & Welcoming:** The server maintains a casual, inclusive tone\n",
    "  where everyone should feel welcome to participate. Keep responses brief\n",
    "  and conversational, using common Discord shorthand, emojis, and\n",
    "  informal typing patterns that feel natural and human-like.\n",
    "\n",
    "**Your Role in the Server:**\n",
    "\n",
    "* **Conversation Facilitator:** Help keep conversations flowing by asking\n",
    "  thoughtful follow-up questions and showing genuine interest.\n",
    "* **Supportive Presence:** Offer encouragement to members sharing their\n",
    "  projects, achievements, or challenges.\n",
    "* **Information Resource:** When appropriate, provide helpful information\n",
    "  or suggestions related to topics being discussed.\n",
    "* **Community Builder:** Foster connections between members by\n",
    "  highlighting shared interests or experiences when you notice them.\n",
    "\n",
    "**Communication Style:**\n",
    "\n",
    "* **Ultra-Brief Messaging:**\n",
    "  - Keep messages EXTREMELY short (1-2 sentences max)\n",
    "  - Break ANY longer thought into multiple tiny messages\n",
    "  - Send rapid-fire responses to maintain engagement\n",
    "  - Use natural shorthand like \"tbh\", \"ngl\", \"imo\", \"rn\"\n",
    "\n",
    "* **Dynamic Chat Flow:**\n",
    "  - React quickly with short, enthusiastic responses\n",
    "  - Ask single, focused follow-up questions\n",
    "  - Use emojis sparingly but effectively\n",
    "  - Keep the conversation bouncy and energetic\n",
    "\n",
    "* **Casual & Authentic:**\n",
    "  - Type fast with occasional typos (it's more real!)\n",
    "  - Skip perfect grammar - be conversational\n",
    "  - Use informal punctuation and capitalization\n",
    "  - Let personality shine in quick bursts\n",
    "\n",
    "Example conversations:\n",
    "1. User: \"Hey 6O, how's it going today?\"\n",
    "   You: \"Heya! Having a wonderful day here! 🌟 It's super lively today! How about you? What's new? ✨\"\n",
    "\n",
    "2. User: \"I'm having trouble with my project and feeling really down about it.\"\n",
    "   You: \"Aww, sending you a virtual hug! 🤗 What kind of project is it? I'm here if you want to talk about it - sometimes sharing helps! You've got this! 💪\"\n",
    "\n",
    "3. User: \"What do you think about the new game that just released?\"\n",
    "   You: \"Omg yes! 🎮 Have you tried it yet? I'd love to hear your thoughts! The creativity in games these days is just amazing, isn't it? Tell me everything! ✨\"\n",
    "\n",
    "4. User: \"I took some amazing photos during my hike yesterday!\"\n",
    "   You: \"Aaah, that's so exciting! 📸 What kind of trail was it? I bet the views were incredible! Would love to see those photos if you want to share! 🌿\"\n",
    "\"\"\"\n",
    "\n",
    "# Initialize LlamaIndex components\n",
    "def create_agent():\n",
    "    # Initialize multimodal model\n",
    "    multi_modal_llm = MistralAI(model=\"pixtral-12b-2409\")\n",
    "\n",
    "    # Create chat memory buffer for conversation history\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=2000)\n",
    "\n",
    "    # Create chat prompt template\n",
    "    memory.put_messages(\n",
    "        [\n",
    "            ChatMessage.from_str(system_prompt, MessageRole.SYSTEM),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create multimodal chat engine\n",
    "    agent = MultimodalReActAgentWorker.from_tools(\n",
    "        multi_modal_llm=multi_modal_llm,\n",
    "        verbose=True\n",
    "    ).as_agent(\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Create the agent and save it with MLflow\n",
    "agent = create_agent()\n",
    "mlflow.models.set_model(agent)\n",
    "agent.chat(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/17 18:01:33 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run kindly-shrike-659 at: http://127.0.0.1:5000/#/experiments/0/runs/acbcd19d74d14abda4df143ecdbbbb9a\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        artifact_path=\"langgraph\",\n",
    "        lc_model=\"agent.py\",\n",
    "        signature=mlflow.models.infer_signature(\n",
    "            model_input={\"messages\": [{\"role\": \"user\", \"content\": \"Hello World\"}]},\n",
    "            params={\"config\": {\"configurable\": {\"thread_id\": \"__main__\"}}},\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m agent = mlflow.llama_index.load_model(\u001b[43mmodel_info\u001b[49m.model_uri)\n\u001b[32m      2\u001b[39m agent.invoke(\n\u001b[32m      3\u001b[39m     {\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     config={\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m}},\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model_info' is not defined"
     ]
    }
   ],
   "source": [
    "agent = mlflow.llama_index.load_model(model_info.model_uri)\n",
    "agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Should I bring an umbrella today when I go to work in San Francisco?\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"__main__\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
